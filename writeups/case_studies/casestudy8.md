# Bridging The Digital Divide With Language Data From Around The World

The United States has been the home of digital technological development over the last 20 years. While there have been efforts across other continents (in particular, East Asia), the [largest eight tech companies by market capitalisation](https://companiesmarketcap.com/gbp/tech/largest-tech-companies-by-market-cap/) all originate from the USA. This convergence has an impact on the types of technology created, and in particular the languages used for developing technology, which is heavily dominated, unsurprisingly, by English. 

More than 7,000 languages are spoken around the world today, but currently, [state-of-art AI LLMs cover only a small percentage](https://cohere.com/research/papers/the-ai-language-gap.pdf) of them, favouring western language and cultural perspectives. In fact, according to [a study by Cohere](https://cohere.com/research/papers/the-ai-language-gap.pdf), 93% of the world’s languages are still not being used to train LLMs. This is in part because many non-English languages are considered ‘low-resource’, meaning they are less prominent within research and lack the high-quality datasets necessary for training language models. The lack of data on under-resourced languages creates a series of consequences. Speakers of under-resourced languages can be left behind as they are unable to use and develop technology, and the lack of linguistic diversity can introduce biases towards western perspectives and views. 

Collecting high quality data for all the languages around the world is difficult. Existing datasets, where they exist, are often low quality, and collecting data can be extremely time- and resource-intensive. This makes development of technology using these datasets more difficult, and the models that are trained on the data perform worse and are more expensive to run. However, new efforts are emerging to curate and collect under-resourced language datasets in collaboration with communities, such as the [Aya model by Cohere](https://cohere.com/research/aya), and the [Lacuna Fund](https://lacunafund.org/), to ensure that every person around the world has equal access to technology. [Common Voice](https://commonvoice.mozilla.org/), an initiative stewarded by Mozilla, is one of the first initiatives to pursue this goal.

## About the use case
Mozilla Foundation established [Common Voice](https://commonvoice.mozilla.org/) in 2017 to help to ‘teach machines how real people speak’. The project seeks to enable people speaking a variety of languages, with varied accents and dialects, to access and use technology. Common Voice is fulfilling this goal by curating a dataset of under-resourced languages, created by the people who speak those languages, and open and accessible to everyone. The transcribed sentences are collected in a voice database available under the public domain license CC0, ensuring that developers can use the database for voice-to-text applications without restrictions or costs. The dataset has been downloaded more than five million times, indicating the huge numbers of people already using the data in different ways. Common Voice is the first step towards the creation of technology in languages not currently served by technological innovation. 

Due to the open nature of the Common Voice platform, many different communities are using the infrastructure and datasets to support particular efforts around different languages and use cases:

* In Catalunya, the [government prioritised the creation of a Catalan dataset](https://aclanthology.org/2024.lrec-main.193.pdf) to safekeep the language in a digitised world. As a result, Catalan is now one of the [most contributed languages on Common Voice](https://www.plataforma-llengua.cat/que-fem/en_noticies/86/catalan-exceeds-3000-hours-of-recording-on-common-voice-and-is-already-the-second-language-with-the-most-minutes-recorded-and-validated).  
* [Masakhane.io](http://Masakhane.io) is a grassroots NLP community for Africa, by Africa. The community’s project, [MakerereNLP](https://www.masakhane.io/ongoing-projects/makererenlp-text-speech-for-east-africa), has created both audio and text datasets for more than five languages spoken in Uganda and Kenya by bringing together the community to contribute to Common Voice.  
* [Mabel.Care](https://mabel.care/) uses Common Voice datasets to develop a real-time translation system for healthcare and the public sectors. One of the first languages the company used was Ukrainian; it built [a translation tool to help Ukrainian refugees interact with Swedish social services](https://www.technologyreview.com/2024/11/15/1106935/how-this-grassroots-effort-could-make-ai-voices-more-diverse/). One challenge they found was that young men featured heavily in the dataset, but were not the main users of the app, demonstrating the need for a diverse set of contributors to each language.

## About the data 
The Common Voice datasets contain 33,151 recorded hours of audio data (22,109 of which are validated), split into datasets for each of the 133 languages. Each data entry also includes demographic metadata about the contributor, such as [age, sex and accent](https://observablehq.com/@kathyreid/mozilla-common-voice-v19-dataset-metadata-coverage%20), which supports accuracy when training speech recognition engines. Catalan, English, Kinyarwanda, Spanish and Belarusian are some of the languages with the highest volumes of data. Languages are consistently being added to the system, such as Sicilian, Tagalog and Kannada.

The data is deliberately curated for and by the communities who speak these languages, by contributing voice and text data in their language. In practice this means that the nuances of the language come through into the dataset, and the benefits of the datasets flow back to the contributors. Furthermore, Common Voice data is licensed under the permissive [Creative Commons CC0 Public Domain Dedication](https://creativecommons.org/publicdomain/zero/1.0/deed.en), ensuring it can be used by anyone under no restrictions.  

## Looking forward
Given the aims of Common Voice, naturally the long term goal is to continue to support more languages on the platform (with 80 new languages set to feature on the platform in 2025). There is much still to do, not only in supporting new languages, but also in ensuring that languages already on the platform have enough data to train AI models effectively. Common Voice plans to reduce friction to participation, so that more people can contribute to the platform as easily as possible (such as offline contributions and [spontaneous contributions](https://commonvoice.mozilla.org/spontaneous-speech/beta/prompts)) to encourage richer datasets. 

Common Voice’s wider strategy to level the playing field in the current era of developing AI models should lead to more voice-enabled solutions, for many more languages. Beyond providing the infrastructure and data to make that happen, Common Voice has a wider role in working strategically to support efforts to curate datasets on under-resourced languages. One particular angle is to explore how governments can deploy data collection platforms like Common Voice as part of their efforts to develop national AI tools and models. 