# Using open data to build open source LLMs for everyone

Large Language Models (LLMs) are now ubiquitous in day-to-day life, predominantly as assistants for people performing research, analysis and engineering in their work. However, not everyone can take advantage of their groundbreaking benefits; especially in regulated industries, LLMs have a few obstacles to overcome before they can be fully adopted.

Firstly, size matters. While LLMs’ [performance increases with their size](https://arxiv.org/abs/2001.08361), so too does cost,  meaning many users of high-performance LLMs require them to be hosted on the cloud. However, when dealing with sensitive or proprietary data, organisations are much less likely to use cloud services: they are obliged to ensure the private data they collect and work with is kept safe, and can rarely permit its processing by a third-party cloud provider. 

Secondly, the legislative environment around LLMs can make their usage difficult. [Foundation models across the field of AI have been found to be insufficiently transparent about their training data](https://theodi.org/insights/reports/the-ai-data-transparency-index/), meaning adopting organisations cannot be sure that LLMs were not trained on intellectual property or copyrighted assets. With this being the subject of ongoing lawsuits, organisations in regulated industries need assurances that the LLMs they are looking to adopt were ethically trained and compliant with legislation.

Pleias, a French AI development lab, is looking to build LLMs that explicitly address these challenges. The team was established in 2024 with the core aim of making LLMs specifically designed to help in enterprise in regulated industries and entirely trained on open data. 

From the start of its work, Pleias took a [data-centric approach](https://theodi.org/insights/projects/data-centric-ai/): for its LLMs to achieve their goals, it believed it was optimal to focus on making high-quality training data. Over 2024, the Pleias team worked to achieve this, not only by curating a selection of sources for open data, but also innovating new methods to process text, clean it, and ensure it is safe and compliant with legislative requirements. This work culminated in Pleias openly releasing the first generation of ‘[Common Models](https://huggingface.co/collections/PleIAs/common-models-674cd0667951ab7c4ef84cc4)’, a set of five models where the largest contained only 1.2 billion parameters. 

This release captured the attention of AI practitioners, with an accompanying press release ‘[They said it couldn’t be done](https://huggingface.co/blog/Pclanglais/common-models)’ containing internal benchmarking results that placed their largest model higher than Llama 3.2-3B, a model developed by Meta that has more than twice the number of parameters, in retrieval-augmented generation tasks the model is designed for. Also in the press release were remarkable results regarding the efficiency and sustainability of Pleias’ training methods.

Before this release, though, Pleias had already publicly released ‘[CommonCorpus](https://huggingface.co/collections/PleIAs/common-corpus-6734e0f67ac3f35e44075f93)’, the custom dataset they developed to train their AI models. This dataset is not as large as [CommonCrawl](https://commoncrawl.org/) or other AI training datasets, but with its high quality and assurances around openness, it has been extremely popular.

## Less can be more with open data  
With the specific objectives it set itself, Pleias is operating under a few constraints: the team is small, their models need to be small, and training data must be collated from open sources. However, constraints breed creativity, and Pleias’ work has plenty of interesting features that make it stand out.

Take, for example, the Pleias approach to AI safety. AI safety is extremely important, with AI developers around the world agreeing that their models cannot under any circumstances exhibit racial or gender-based bias, provide misinformation or restricted information, or put humans and the environment at risk. To conform to these rules, AI developers typically fine-tune LLMs (using [Reinforcement Learning with Human Feedback \- RLHF](https://arxiv.org/abs/2203.02155)) after they have been trained on the entire internet, with global teams of staff working around the clock to prompt the models and reward safe responses (while punishing unsafe behaviour).

This approach was out of the question for Pleias \- not only because of its small team size, but also because it is orthogonal to their data-centric principles. Training on a massive dataset like the entire internet means that unsafe, dangerous text could be taught to the LLMs, which is why RLHF is required. Meanwhile, if Pleias’ smaller, curated dataset, which is entirely open data, were to be vetted to exclude any dangerous or harmful information, the LLM does not learn how to be unsafe, removing the need for extensive RLHF and the cost that comes with it. To do this vetting automatically, Pleias uses its openly released [Celadon](https://huggingface.co/PleIAs/celadon) model, which was trained on another of its open datasets ([ToxicCommons](https://huggingface.co/datasets/PleIAs/ToxicCommons)).

Celadon is just one example of the team’s innovation. Another example is the [Bad Data Toolbox](https://huggingface.co/collections/PleIAs/bad-data-toolbox-66981c2d0df662459252844e), a set of models Pleias designed to help process open historical data that has been poorly digitised. Additionally, Pleias developed a [unique tokenising algorithm](https://arxiv.org/abs/2409.04599) as a competitor to the most-used techniques, aiming to address common problems that affect LLMs in downstream tasks. Each of these innovations has an impact; by releasing them openly, AI practitioners can use them for their own work, ensuring equity across the field of AI and encouraging the use of open data practices.

## About the data  
Pleias’ training data for its LLMs, called [Common Corpus](https://huggingface.co/collections/PleIAs/common-corpus-6734e0f67ac3f35e44075f93), is an aggregate dataset with five key components, from across culture, government, open source, science, and the web. Each data point within these components is assured to be permissively licensed, which assuages the concerns of even those in strictly regulated industries. The dataset is also multilingual (for seven languages) and has been cleaned by Pleias with Bad Data Toolbox and Celadon.

The Common Corpus is openly available on [Hugging Face](https://huggingface.co/), making it easily accessible to all AI developers. Importantly, with this open publishing, each row of the dataset is highly visible, making it suitably auditable for legislators or other concerned parties. Pleias further enhances explainability by ensuring that licence and traceability information is present in the dataset. 

## Looking forward  
In February 2025, the [AI Action Summit](https://www.elysee.fr/en/sommet-pour-l-action-sur-l-ia) took place in Paris, the home of Pleias. At the summit special attention was placed on the practicalities of AI, with participants speaking about LLMs’ size, sustainability, and ethics when it comes to intellectual property. Pleias are ahead of this curve and provide an important, concrete example that could be used to frame discussions and look towards the future of AI.

In the long run, what might Pleias’ impact look like? A good example would be in healthcare. Medical organisations have a lot of moving parts, with data constantly being collected and a need for analyses and decisions to be made quickly. LLMs could help, acting as assistants or [agents](https://www.promptingguide.ai/research/llm-agents) within an organisation to support the delivery of healthcare. However, with strict rules about patient data and tight budgets, cloud-based LLMs are out of the question, and the organisations also require assurances about their ethics, safety, and potential IP violations. Pleias solves this problem with small-scale models that are transparent and, importantly, entirely trained on open data. 